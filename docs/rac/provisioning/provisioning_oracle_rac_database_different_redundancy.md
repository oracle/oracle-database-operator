#  Provisioning an Oracle RAC Database with differnet ASM disk groups for CRS and RDBMS with different redundancy levels

Deploy an Oracle RAC Database and Grid Infrastructure using Oracle RAC Controller with separate ASM disk groups for CRS and RDBMS, each with its own redundancy level, using Kubernetes.


#### Use Case
* In this use case, the Oracle Grid Infrastructure and Oracle RAC Database are deployed automatically using Oracle RAC Controller. The responsefile is generated by the Oracle RAC Controller based on the input parameters specified in the `.yaml` file. 
* In this case, the Oracle RAC Database is deployed with with different diskgroups for CRS and RDBMS. 
* The diskgroups have different redundancy levels. 
* This example uses `racdb_prov_diff_dg_for_db_and_crs_with_diff_redundancy.yaml` to provision an Oracle RAC Database using Oracle RAC Controller. The provisioning includes:
  * Two Kubernetes Pods as the RAC Nodes
  * Headless services for RAC
    * VIP Service
    * Scan Service
    * RAC Node hostname
  * Shared Persistent Volumes created automatically based on specified shared disks for Oracle RAC shared storage(ASM)
  * Software Persistent Volumes and Staged Software Persistent Volumes using the specified location on the corresponding worker nodes
  * Namespace: `rac`
  * Staged Software location on the worker nodes is specified by `hostSwStageLocation`. Grid Infrastructure and RDBMS Binaries are copied to this location on the worker nodes. 
  * Software location on the worker nodes is specified by `racHostSwLocation`. The GI HOME and the RDBMS HOME in the Oracle RAC Pods will be mounted using this location on the corresponding worker node. 
  * Diskgroup named `CRSDATA` for the CRS with `EXTERNAL` redundancy level 
  * Diskgroup named `DATA` for Database Files with `NORMAL` redundancy level 
  * Diskgroup named `RECO` as the default location for fast recovery area with `EXTERNAL` redundancy level 
  * Diskgroup named `REDO` for Oracle-managed control files and online redo logs. This diskgroup has `EXTERNAL` redundancy level 
  
### Example Details 

  * This example uses a pre-built Oracle RAC Database slim image available on Oracle OCIR: `phx.ocir.io/intsanjaysingh/db-repo/oracle/database-rac:19.3.0-slim` is used. 
  * If you plan to build the image yourself, then you can build using the files from this [GitHub location](https://github.com/oracle/docker-images/tree/main/OracleDatabase/RAC/OracleRealApplicationClusters#building-oracle-rac-database-container-slim-image). In this case, you will need to change value of `image` with the image that you have built in your enviornment in file `racdb_prov_diff_dg_for_db_and_crs.yaml`.   
  * The ASM diskgroup is configured using the shared disks on the worker nodes:  
    * `/dev/disk/by-partlabel/qck-ocne19-asmdisk1` and `/dev/disk/by-partlabel/qck-ocne19-asmdisk2` for the `CRSDATA` diskgroup 
    * `/dev/disk/by-partlabel/qck-ocne19-asmdisk3`,`/dev/disk/by-partlabel/qck-ocne19-asmdisk4`,`/dev/disk/by-partlabel/qck-ocne19-asmdisk5` and `/dev/disk/by-partlabel/qck-ocne19-asmdisk6` for the `DATA` diskgroup 
    * `/dev/disk/by-partlabel/qck-ocne19-asmdisk7` and `/dev/disk/by-partlabel/qck-ocne19-asmdisk8` for the `RECO` diskgroup 
    * `/dev/disk/by-partlabel/qck-ocne19-asmdisk9` and `/dev/disk/by-partlabel/qck-ocne19-asmdisk10` for the `REDO` diskgroup            
  * These disks are specified using parameter `asmDiskGroupDetails` in the YAML file.   

### Steps: Deploy a Two-Node Oracle RAC Database with Different Disk Groups and Different Redundancy Levels
Use the following file: [racdb_prov_diff_dg_for_db_and_crs_with_diff_redundancy.yaml](./racdb_prov_diff_dg_for_db_and_crs_with_diff_redundancy.yaml) 

Complete these steps:

1. Deploy the `racdb_prov_diff_dg_for_db_and_crs_with_diff_redundancy.yaml` file:
    ```sh
    kubectl apply -f racdb_prov_diff_dg_for_db_and_crs_with_diff_redundancy.yaml
    ```
2. Check the status of the deployment:
    ```sh
    # Check the status of the Kubernetes Pods:    
    kubectl get all -n rac
    
    # Check the logs of a particular pod. For example, to check status of pod "racnode1-0":    
    kubectl exec -it pod/racnode1-0 -n rac -- bash -c "tail -f /tmp/orod/oracle_db_setup.log"
    ===================================
    ORACLE RAC DATABASE IS READY TO USE
    ===================================
    ```
3. After the deployment is completed, check the details of the disk groups:
    ```sh
    # Switch to one of the pod for RAC Nodes:    
    kubectl exec -it pod/racnode1-0 -n rac -- bash

    # Switch to the CRS user i.e. "grid"
    su - grid
    
    # Check the details of the diskgroups:
    [grid@racnode1-0 ~]$ asmcmd lsdg
    State    Type    Rebal  Sector  Logical_Sector  Block       AU  Total_MB  Free_MB  Req_mir_free_MB  Usable_file_MB  Offline_disks  Voting_files  Name
    MOUNTED  EXTERN  N         512             512   4096  4194304    102392   102040                0          102040              0             Y  CRSDATA/
    MOUNTED  NORMAL  N         512             512   4096  1048576    204792   191624            51198           70213              0             N  DATA/
    MOUNTED  EXTERN  N         512             512   4096  1048576    102396   102283                0          102283              0             N  RECO/
    MOUNTED  EXTERN  N         512             512   4096  1048576    102396    98156                0           98156              0             N  REDO/
    ```
4. When the YAML file in this example is applied, the example logs are in [Logs](./logs/racdb_prov_diff_dg_diff_redundancy/racdbprov-sample_details.txt). The corresponding [DB Operator Logs](./logs/racdb_prov_diff_dg_diff_redundancy/operator_logs.txt). 
